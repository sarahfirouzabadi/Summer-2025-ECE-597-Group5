{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sarahfirouzabadi/Summer-2025-ECE-597-Group5/blob/main/Summer_2025_ECE_597_Group5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "9IhV6MzuAM7y"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import nltk\n",
        "from collections import Counter\n",
        "from nltk.corpus import stopwords\n",
        "import re\n",
        "from nltk.sentiment import SentimentIntensityAnalyzer\n",
        "from textblob import Word, TextBlob\n",
        "from scipy.sparse import hstack, csr_matrix\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import roc_auc_score, roc_curve, classification_report, auc, accuracy_score, confusion_matrix, ConfusionMatrixDisplay\n",
        "from sklearn.utils import resample\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from wordcloud import WordCloud\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "TFIDV=TfidfVectorizer()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "CEAS=pd.read_csv('/CEAS_08.csv')\n",
        "CEAS.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 304
        },
        "id": "mDoVS2hwAY7t",
        "outputId": "5b6651c7-d4e0-4668-8786-9549c57b9bc2"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/CEAS_08.csv'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2-3865932537.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mCEAS\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/CEAS_08.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mCEAS\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1619\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1620\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1879\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1880\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1881\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 873\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    874\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/CEAS_08.csv'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "label_counts = CEAS['label'].value_counts().sort_index()\n",
        "num_ham = label_counts.get(0, 0)\n",
        "num_spam = label_counts.get(1, 0)\n",
        "\n",
        "print(f\"CEAS:\\nHam: {num_ham}\\nSpam: {num_spam}\")"
      ],
      "metadata": {
        "id": "xdXRFs7yXooh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "email_counts = CEAS['label'].value_counts().rename({0: 'Normal', 1: 'Spam'})\n",
        "\n",
        "email_counts.plot.pie(\n",
        "    autopct='%1.1f%%',\n",
        "    colors=['#66b3ff','#ff6666'],\n",
        "    startangle=90,\n",
        "    explode=(0, 0.1),\n",
        "    shadow=True\n",
        ")\n",
        "\n",
        "plt.title('Percentage of Spam vs Normal Emails')\n",
        "plt.ylabel('')  # remove default y-axis label\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "h5MJgwuIYEf8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "SpamAssasin = pd.read_csv('/SpamAssasin.csv')\n",
        "SpamAssasin.head()"
      ],
      "metadata": {
        "id": "uLIO3sPiGV84"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "label_counts = SpamAssasin['label'].value_counts().sort_index()\n",
        "num_ham = label_counts.get(0, 0)\n",
        "num_spam = label_counts.get(1, 0)\n",
        "\n",
        "print(f\"SpamAssasin:\\nHam: {num_ham}\\nSpam: {num_spam}\")"
      ],
      "metadata": {
        "id": "FSFFvGeHYVWD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "email_counts = SpamAssasin['label'].value_counts().rename({0: 'Normal', 1: 'Spam'})\n",
        "\n",
        "email_counts.plot.pie(\n",
        "    autopct='%1.1f%%',\n",
        "    colors=['#66b3ff','#ff6666'],\n",
        "    startangle=90,\n",
        "    explode=(0, 0.1),\n",
        "    shadow=True\n",
        ")\n",
        "\n",
        "plt.title('Percentage of Spam vs Normal Emails')\n",
        "plt.ylabel('')\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "OjXYAAbpYgVO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Nazario = pd.read_csv('/Nazario.csv')\n",
        "Nazario.head()"
      ],
      "metadata": {
        "id": "LxpTD-UjG5be"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "label_counts = Nazario['label'].value_counts().sort_index()\n",
        "num_ham = label_counts.get(0, 0)\n",
        "num_spam = label_counts.get(1, 0)\n",
        "\n",
        "print(f\"Nazario:\\nHam: {num_ham}\\nSpam: {num_spam}\")"
      ],
      "metadata": {
        "id": "fOpLoP7BYq1G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "email_counts = Nazario['label'].value_counts().rename({0: 'Normal', 1: 'Spam'})\n",
        "\n",
        "email_counts.plot.pie(\n",
        "    autopct='%1.1f%%',\n",
        "    colors=['#ff6666'],\n",
        "    startangle=90,\n",
        "    shadow=True\n",
        ")\n",
        "\n",
        "plt.title('Percentage of Spam vs Normal Emails')\n",
        "plt.ylabel('')\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "k75bUoMwYy5Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Nigerian_Fraud = pd.read_csv('/Nigerian_Fraud.csv')\n",
        "Nigerian_Fraud.head()"
      ],
      "metadata": {
        "id": "4bwlSBumHpn4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "label_counts = Nigerian_Fraud['label'].value_counts().sort_index()\n",
        "num_ham = label_counts.get(0, 0)\n",
        "num_spam = label_counts.get(1, 0)\n",
        "\n",
        "print(f\"Nigerian_Fraud:\\nHam: {num_ham}\\nSpam: {num_spam}\")"
      ],
      "metadata": {
        "id": "nMogMwNsYxjM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "email_counts = Nigerian_Fraud['label'].value_counts().rename({0: 'Normal', 1: 'Spam'})\n",
        "\n",
        "email_counts.plot.pie(\n",
        "    autopct='%1.1f%%',\n",
        "    colors=['#ff6666'],\n",
        "    startangle=90,\n",
        "    shadow=True\n",
        ")\n",
        "\n",
        "plt.title('Percentage of Spam vs Normal Emails')\n",
        "plt.ylabel('')\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "DlRWevUIY2e3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Combine datasets and removing unnecessary coloumns\n",
        "combined_df = pd.concat([\n",
        "    CEAS[['sender', 'subject', 'body', 'label']],\n",
        "    SpamAssasin[['sender', 'subject', 'body', 'label']],\n",
        "    Nazario[['sender', 'subject', 'body', 'label']],\n",
        "    Nigerian_Fraud[['sender', 'subject', 'body', 'label']]\n",
        "], ignore_index=True)\n",
        "\n",
        "print(combined_df.head())\n",
        "print(f\"Combined DataFrame shape: {combined_df.shape}\")"
      ],
      "metadata": {
        "id": "Ko6t96nYIzK5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove duplicates\n",
        "combined_df.drop_duplicates(subset=['sender', 'subject', 'body'], inplace=True)\n",
        "\n",
        "# Missing values\n",
        "print(combined_df.isnull().sum())\n",
        "\n",
        "# Drop rows with missing sender, subject, or body\n",
        "combined_df.dropna(subset=['sender', 'subject', 'body'], inplace=True)\n",
        "\n",
        "combined_df.reset_index(drop=True, inplace=True)\n",
        "print(f'Cleaned dataset shape: {combined_df.shape}')"
      ],
      "metadata": {
        "id": "WqdhSKgoMLsl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Count spam and normal emails\n",
        "email_counts = combined_df['label'].value_counts().rename({0: 'Normal', 1: 'Spam'})\n",
        "\n",
        "# Plot a pie chart\n",
        "email_counts.plot.pie(\n",
        "    autopct='%1.1f%%',\n",
        "    colors=['#66b3ff','#ff6666'],\n",
        "    startangle=90,\n",
        "    explode=(0, 0.1),\n",
        "    shadow=True\n",
        ")\n",
        "\n",
        "plt.title('Percentage of Spam vs Normal Emails')\n",
        "plt.ylabel('')  # remove default y-axis label\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "y3cc_d_9MR7S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Down sampling (Reducing spam size)"
      ],
      "metadata": {
        "id": "qH9WeurlPEc4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "spam = combined_df[combined_df.label == 1]\n",
        "normal = combined_df[combined_df.label == 0]\n",
        "\n",
        "# Downsample spam class to match normal class size\n",
        "spam_downsampled = resample(spam,\n",
        "                            replace=False,\n",
        "                            n_samples=len(normal),\n",
        "                            random_state=42)\n",
        "\n",
        "# Combine them all again\n",
        "balanced_df = pd.concat([spam_downsampled, normal])\n",
        "\n",
        "# Shuffle rows\n",
        "balanced_df = balanced_df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
        "\n",
        "\n",
        "print(balanced_df['label'].value_counts())"
      ],
      "metadata": {
        "id": "jtQzI2ezOccy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "balanced_df.info()\n",
        "balanced_df.head()"
      ],
      "metadata": {
        "id": "z4g_DiBhPX3j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sns.countplot(x='label', data=balanced_df)\n",
        "plt.title('Spam vs Normal Emails')\n",
        "plt.xticks([0, 1], ['Normal', 'Spam'])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ZkwKW6dgPiEv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "balanced_df['email_length'] = balanced_df['body'].fillna('').str.len()\n",
        "\n",
        "filtered_df = balanced_df[balanced_df['email_length'] < 5000]\n",
        "\n",
        "# Plot the filtered data\n",
        "sns.histplot(data=filtered_df, x='email_length', hue='label', bins=50, kde=True)\n",
        "plt.title('Distribution of Email Lengths (filtered under 5000 characters)')\n",
        "plt.xlabel('Email Length (characters)')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "fIIyhQdxQodI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "spam_words = ' '.join(balanced_df[balanced_df['label']==1]['body'].dropna())\n",
        "wordcloud_spam = WordCloud(width=800, height=400, max_words=100).generate(spam_words)\n",
        "\n",
        "plt.figure(figsize=(10,5))\n",
        "plt.imshow(wordcloud_spam, interpolation='bilinear')\n",
        "plt.title('Spam Emails - Frequent Words')\n",
        "plt.axis('off')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "E8Z7DXOMTyF-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Download stopwords once\n",
        "nltk.download('stopwords')\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "# Initialize counter\n",
        "word_counter = Counter()\n",
        "\n",
        "# Loop through spam emails line by line (avoids joining the full string)\n",
        "for text in balanced_df[balanced_df['label'] == 1]['body'].dropna():\n",
        "    # Lowercase, remove punctuation/numbers, tokenize\n",
        "    words = re.findall(r'\\b[a-z]+\\b', text.lower())\n",
        "\n",
        "    # Remove stopwords\n",
        "    filtered_words = [word for word in words if word not in stop_words]\n",
        "\n",
        "    # Update counter\n",
        "    word_counter.update(filtered_words)\n",
        "\n",
        "# Show top 50\n",
        "print(\"Top 50 frequent words in spam emails:\")\n",
        "for word, freq in word_counter.most_common(50):\n",
        "    print(f\"{word}: {freq}\")"
      ],
      "metadata": {
        "id": "zs5CcuIQUVub"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize(text):\n",
        "    text = str(text).lower()\n",
        "    words = re.findall(r'\\b[a-z]+\\b', text)  # only alphabetic words\n",
        "    return [w for w in words if w not in stop_words]\n",
        "\n",
        "# Apply tokenization\n",
        "balanced_df['tokens'] = balanced_df['body'].apply(tokenize)\n",
        "\n",
        "# Separate phishing and normal\n",
        "phishing_tokens = balanced_df[balanced_df['label'] == 1]['tokens']\n",
        "normal_tokens = balanced_df[balanced_df['label'] == 0]['tokens']\n",
        "\n",
        "# Average words\n",
        "avg_words_phishing = phishing_tokens.apply(len).mean()\n",
        "avg_words_normal = normal_tokens.apply(len).mean()\n",
        "\n",
        "# Flatten token lists\n",
        "flat_phishing = [word for tokens in phishing_tokens for word in tokens]\n",
        "flat_normal = [word for tokens in normal_tokens for word in tokens]\n",
        "\n",
        "# Unique word counts\n",
        "unique_phishing = len(set(flat_phishing))\n",
        "unique_normal = len(set(flat_normal))\n",
        "\n",
        "# Most indicative phishing words (appearing more in phishing than in normal)\n",
        "phishing_counter = Counter(flat_phishing)\n",
        "normal_counter = Counter(flat_normal)\n",
        "\n",
        "indicative_phishing = {\n",
        "    word: freq for word, freq in phishing_counter.items()\n",
        "    if freq > normal_counter.get(word, 0)\n",
        "}\n",
        "\n",
        "top_indicative = Counter(indicative_phishing).most_common(20)\n",
        "\n",
        "print(f\"Average words per phishing email: {avg_words_phishing:.2f}\")\n",
        "print(f\"Total unique words in phishing emails: {unique_phishing}\")\n",
        "\n",
        "print(f\"Average words per normal email: {avg_words_normal:.2f}\")\n",
        "print(f\"Total unique words in normal emails: {unique_normal}\")\n",
        "\n",
        "print(\"\\nTop 20 words most indicative of phishing:\")\n",
        "for word, freq in top_indicative:\n",
        "    print(f\"{word}: {freq}\")"
      ],
      "metadata": {
        "id": "O-Ovx4zBdBP0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Average words data\n",
        "avg_words = {'Phishing': 120.63, 'Normal': 215.75}\n",
        "\n",
        "plt.figure(figsize=(6, 4))\n",
        "sns.barplot(x=list(avg_words.keys()), y=list(avg_words.values()))\n",
        "plt.title('Average Words per Email')\n",
        "plt.ylabel('Average Word Count')\n",
        "plt.ylim(0, max(avg_words.values()) + 50)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "BnhWXJcgfaIp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Unique words data\n",
        "unique_words = {'Phishing': 81119, 'Normal': 105622}\n",
        "\n",
        "plt.figure(figsize=(6, 4))\n",
        "sns.barplot(x=list(unique_words.keys()), y=list(unique_words.values()))\n",
        "plt.title('Total Unique Words')\n",
        "plt.ylabel('Unique Word Count')\n",
        "plt.ylim(0, max(unique_words.values()) * 1.1)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "MWYoLV3nfZPW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Subject"
      ],
      "metadata": {
        "id": "Ga1qFsKmj3zL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "balanced_df['subject_length'] = balanced_df['subject'].fillna('').apply(len)\n",
        "\n",
        "plt.figure(figsize=(6,4))\n",
        "sns.histplot(data=balanced_df, x='subject_length', hue='label', bins=50, kde=True)\n",
        "plt.title('Distribution of Subject Lengths')\n",
        "plt.xlabel('Subject Length (characters)')\n",
        "plt.ylabel('Email Count')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "26IooUv4j26V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Filter out very long subjects\n",
        "filtered_subject_df = balanced_df[balanced_df['subject_length'] < 200]\n",
        "\n",
        "plt.figure(figsize=(6,4))\n",
        "sns.histplot(data=filtered_subject_df, x='subject_length', hue='label', bins=50, kde=True)\n",
        "plt.title('Distribution of Subject Lengths (Filtered < 200 chars)')\n",
        "plt.xlabel('Subject Length (characters)')\n",
        "plt.ylabel('Email Count')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "prpjNHGxkHtS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "avg_len_spam = balanced_df[balanced_df['label'] == 1]['subject_length'].mean()\n",
        "avg_len_normal = balanced_df[balanced_df['label'] == 0]['subject_length'].mean()\n",
        "\n",
        "print(f\"Average subject length - Phishing: {avg_len_spam:.2f}\")\n",
        "print(f\"Average subject length - Normal: {avg_len_normal:.2f}\")"
      ],
      "metadata": {
        "id": "Hf4xEyDmkONp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sns.barplot(x=['Phishing', 'Normal'], y=[avg_len_spam, avg_len_normal])\n",
        "plt.title('Average Subject Length')\n",
        "plt.ylabel('Characters')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "a2d4HjKAkg1s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "def clean_subject(text):\n",
        "    words = re.findall(r'\\b[a-z]+\\b', str(text).lower())\n",
        "    return [w for w in words if w not in stop_words]\n",
        "\n",
        "# Tokenize spam and normal subjects\n",
        "spam_subject_words = balanced_df[balanced_df['label'] == 1]['subject'].dropna().apply(clean_subject)\n",
        "normal_subject_words = balanced_df[balanced_df['label'] == 0]['subject'].dropna().apply(clean_subject)\n",
        "\n",
        "# Flatten\n",
        "spam_subject_flat = [word for words in spam_subject_words for word in words]\n",
        "normal_subject_flat = [word for words in normal_subject_words for word in words]\n",
        "\n",
        "# Count top words\n",
        "spam_top_subject = Counter(spam_subject_flat).most_common(15)\n",
        "normal_top_subject = Counter(normal_subject_flat).most_common(15)\n",
        "\n",
        "# For spam\n",
        "df_spam_subject = pd.DataFrame(spam_top_subject, columns=['Word', 'Count'])\n",
        "sns.barplot(data=df_spam_subject, y='Word', x='Count')\n",
        "plt.title('Top Words in Phishing Email Subjects')\n",
        "plt.show()\n",
        "\n",
        "# For normal\n",
        "df_normal_subject = pd.DataFrame(normal_top_subject, columns=['Word', 'Count'])\n",
        "sns.barplot(data=df_normal_subject, y='Word', x='Count')\n",
        "plt.title('Top Words in Normal Email Subjects')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "GNR-f1Lbkjz3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Extracting the Domain"
      ],
      "metadata": {
        "id": "4xSJCF3sCmCr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "balanced_df[\"sender_2\"]=balanced_df['sender'].str.split('@').str[1]"
      ],
      "metadata": {
        "id": "If2x8yMiCTG_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "balanced_df[\"sender_2\"].value_counts()"
      ],
      "metadata": {
        "id": "naXF-fWlCteH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data cleaning"
      ],
      "metadata": {
        "id": "u8H5GXJYnBbz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_and_tokenize(text):\n",
        "    text = str(text).lower()                             # Lowercase\n",
        "    text = re.sub(r'[^a-z\\s]', '', text)                 # Remove punctuation and numbers\n",
        "    tokens = text.split()                                # Tokenize by whitespace\n",
        "    tokens = [t for t in tokens if t not in stop_words]  # Remove stopwords\n",
        "    return tokens\n",
        "\n",
        "\n",
        "balanced_df['body_tokens'] = balanced_df['body'].fillna('').apply(clean_and_tokenize)\n",
        "balanced_df['subject_tokens'] = balanced_df['subject'].fillna('').apply(clean_and_tokenize)\n",
        "\n",
        "print(\"Sample cleaned body:\", balanced_df['body_tokens'].iloc[0])\n",
        "print(\"Sample cleaned subject:\", balanced_df['subject_tokens'].iloc[0])"
      ],
      "metadata": {
        "id": "K1ZVPUHvnAd0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Feature engineering"
      ],
      "metadata": {
        "id": "uM-QjRKGpcwX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Email body length\n",
        "balanced_df['email_length'] = balanced_df['body'].fillna('').str.len()\n",
        "\n",
        "# Subject length\n",
        "balanced_df['subject_length'] = balanced_df['subject'].fillna('').str.len()\n",
        "\n",
        "# Number of words in the body\n",
        "balanced_df['body_word_count'] = balanced_df['body_tokens'].apply(len)\n",
        "\n",
        "# Number of words in the subject\n",
        "balanced_df['subject_word_count'] = balanced_df['subject_tokens'].apply(len)\n",
        "\n",
        "# Number of URLs\n",
        "url_pattern = r'(https?://[^\\s]+)' # Define a URL regex pattern\n",
        "balanced_df['num_urls'] = balanced_df['body'].fillna('').apply(lambda x: len(re.findall(url_pattern, x)))\n",
        "\n",
        "# Sender domain\n",
        "balanced_df['sender_domain'] = balanced_df['sender'].fillna('').str.extract(r'@([\\w\\.-]+)')\n",
        "\n",
        "# Preview engineered features\n",
        "balanced_df[['email_length', 'subject_length', 'body_word_count', 'subject_word_count',\n",
        "             'num_urls', 'sender_domain']].head()\n"
      ],
      "metadata": {
        "id": "xKlwu1aenyGa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "TF-IDF"
      ],
      "metadata": {
        "id": "rCnHcnQFrNrf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Join tokens back to strings\n",
        "balanced_df['body_clean'] = balanced_df['body_tokens'].apply(lambda x: ' '.join(x))\n",
        "balanced_df['subject_clean'] = balanced_df['subject_tokens'].apply(lambda x: ' '.join(x))\n",
        "balanced_df['sender_domain_clean'] = balanced_df['sender_domain'].fillna('')\n",
        "\n",
        "# body\n",
        "tfidf_body = TfidfVectorizer(stop_words='english', max_features=3000)\n",
        "X_body = tfidf_body.fit_transform(balanced_df['body_clean'])\n",
        "\n",
        "# subject\n",
        "tfidf_subject = TfidfVectorizer(stop_words='english', max_features=1000)\n",
        "X_subject = tfidf_subject.fit_transform(balanced_df['subject_clean'])\n",
        "\n",
        "# sender domain\n",
        "tfidf_sender = TfidfVectorizer()\n",
        "X_sender = tfidf_sender.fit_transform(balanced_df['sender_domain_clean'])\n",
        "\n",
        "print(\"TF-IDF shape (body):\", X_body.shape)\n",
        "print(\"TF-IDF shape (subject):\", X_subject.shape)\n",
        "print(\"TF-IDF shape (sender):\", X_sender.shape)"
      ],
      "metadata": {
        "id": "jDsDoqUKrfvV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train Logistic Regression and Random Forest"
      ],
      "metadata": {
        "id": "PoPf0J0rzHeg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y = balanced_df['label']\n",
        "\n",
        "numeric_cols = [\n",
        "    'email_length',\n",
        "    'subject_length',\n",
        "    'body_word_count',\n",
        "    'subject_word_count',\n",
        "    'num_urls'\n",
        "]\n",
        "X_numeric_raw = balanced_df[numeric_cols].fillna(0)\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_numeric = scaler.fit_transform(X_numeric_raw)\n",
        "\n",
        "\n",
        "X_all = hstack([\n",
        "    X_body,\n",
        "    X_subject,\n",
        "    X_sender,\n",
        "    csr_matrix(X_numeric)\n",
        "])\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X_all, y, test_size=0.2, random_state=42, stratify=y\n",
        ")"
      ],
      "metadata": {
        "id": "yAptgNfaszPz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import cross_val_score, StratifiedKFold\n",
        "\n",
        "# Cross-validation\n",
        "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "log_model_cv = LogisticRegression(max_iter=1000)\n",
        "\n",
        "# Evaluate AUC across 5 folds\n",
        "cv_auc_scores = cross_val_score(log_model_cv, X_all, y, cv=cv, scoring='roc_auc')\n",
        "\n",
        "print(\"Logistic Regression Cross-Validation AUC scores:\", cv_auc_scores)\n",
        "print(f\"Mean AUC: {cv_auc_scores.mean():.4f}, Std: {cv_auc_scores.std():.4f}\")\n"
      ],
      "metadata": {
        "id": "DEgIDd5cYIQg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Logistic Regression:\n",
        "\n",
        "log_model = LogisticRegression(max_iter=1000)\n",
        "log_model.fit(X_train, y_train)\n",
        "\n",
        "# Evaluating\n",
        "y_pred_log = log_model.predict(X_test)\n",
        "y_proba_log = log_model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "print(\"Logistic Regression:\")\n",
        "print(classification_report(y_test, y_pred_log))\n",
        "print(\"ROC AUC Score:\", roc_auc_score(y_test, y_proba_log))"
      ],
      "metadata": {
        "id": "4RAO2lbizl43"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ROC for Logistic Regression\n",
        "fpr_log, tpr_log, _ = roc_curve(y_test, y_proba_log)\n",
        "auc_log = roc_auc_score(y_test, y_proba_log)\n",
        "\n",
        "plt.figure(figsize=(6, 5))\n",
        "plt.plot(fpr_log, tpr_log, label=f'AUC = {auc_log:.4f}')\n",
        "plt.plot([0, 1], [0, 1], 'k--')  # diagonal line\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('ROC Curve - Logistic Regression')\n",
        "plt.legend(loc='lower right')\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "0sOjjYje3-nU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Confusion matrix for Logistic Regression\n",
        "cm_log = confusion_matrix(y_test, y_pred_log)\n",
        "\n",
        "disp_log = ConfusionMatrixDisplay(confusion_matrix=cm_log, display_labels=[\"Normal\", \"Spam\"])\n",
        "disp_log.plot(cmap='Blues', values_format='d')\n",
        "plt.title(\"Confusion Matrix - Logistic Regression\")\n",
        "plt.grid(False)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "f9VGyif48vMe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Random Forest model\n",
        "rf_model = RandomForestClassifier(\n",
        "    n_estimators=100,       # number of trees\n",
        "    random_state=42,        # ensures reproducibility\n",
        "    n_jobs=-1               # uses all CPU cores for training\n",
        ")\n",
        "rf_model.fit(X_train, y_train)\n",
        "\n",
        "y_pred_rf = rf_model.predict(X_test)\n",
        "y_proba_rf = rf_model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "print(\"Random Forest:\")\n",
        "print(classification_report(y_test, y_pred_rf))\n",
        "print(\"ROC AUC Score:\", roc_auc_score(y_test, y_proba_rf))"
      ],
      "metadata": {
        "id": "FQlWLZ331ynW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ROC for Random Forest\n",
        "fpr_rf, tpr_rf, _ = roc_curve(y_test, y_proba_rf)\n",
        "auc_rf = roc_auc_score(y_test, y_proba_rf)\n",
        "\n",
        "plt.figure(figsize=(6, 5))\n",
        "plt.plot(fpr_rf, tpr_rf, label=f'AUC = {auc_rf:.4f}', color='green')\n",
        "plt.plot([0, 1], [0, 1], 'k--')  # diagonal line\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('ROC Curve - Random Forest')\n",
        "plt.legend(loc='lower right')\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "utVj0wEd3jCp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Confusion matrix for Random Forest\n",
        "cm_rf = confusion_matrix(y_test, y_pred_rf)\n",
        "\n",
        "disp_rf = ConfusionMatrixDisplay(confusion_matrix=cm_rf, display_labels=[\"Normal\", \"Spam\"])\n",
        "disp_rf.plot(cmap='Greens', values_format='d')\n",
        "plt.title(\"Confusion Matrix - Random Forest\")\n",
        "plt.grid(False)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "qzRGRwwA8UWO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "comparison_data = {\n",
        "    \"Model\": [\"Logistic Regression\", \"Random Forest\"],\n",
        "    \"Accuracy\": [log_model.score(X_test, y_test), rf_model.score(X_test, y_test)],\n",
        "    \"ROC_AUC\": [roc_auc_score(y_test, y_proba_log), roc_auc_score(y_test, y_proba_rf)],\n",
        "}\n",
        "pd.DataFrame(comparison_data)"
      ],
      "metadata": {
        "id": "wefehari96IH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get numeric feature importances from Random Forest\n",
        "importances = rf_model.feature_importances_[-X_numeric.shape[1]:]  # last features are numeric\n",
        "feature_names = numeric_cols\n",
        "\n",
        "# Plot\n",
        "plt.figure(figsize=(6, 4))\n",
        "sns.barplot(x=importances, y=feature_names)\n",
        "plt.title(\"Numeric Feature Importances (Random Forest)\")\n",
        "plt.xlabel(\"Importance Score\")\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "v6dIFhcvBjt0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Naive Bayes model\n",
        "nb_model = GaussianNB()\n",
        "nb_model.fit(X_train.toarray(), y_train)  # Convert sparse matrix to dense\n",
        "\n",
        "y_pred_nb = nb_model.predict(X_test.toarray())\n",
        "y_proba_nb = nb_model.predict_proba(X_test.toarray())[:, 1]\n",
        "\n",
        "print(\"Naive Bayes:\")\n",
        "print(classification_report(y_test, y_pred_nb))\n",
        "print(\"ROC AUC Score:\", roc_auc_score(y_test, y_proba_nb))\n",
        "\n",
        "# ROC\n",
        "fpr_nb, tpr_nb, _ = roc_curve(y_test, y_proba_nb)\n",
        "auc_nb = roc_auc_score(y_test, y_proba_nb)\n",
        "\n",
        "plt.figure(figsize=(6, 5))\n",
        "plt.plot(fpr_nb, tpr_nb, label=f'AUC = {auc_nb:.4f}', color='blue')\n",
        "plt.plot([0, 1], [0, 1], 'k--')  # diagonal line\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('ROC Curve - Naive Bayes')\n",
        "plt.legend(loc='lower right')\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "# Confusion matrix\n",
        "cm_nb = confusion_matrix(y_test, y_pred_nb)\n",
        "\n",
        "disp_nb = ConfusionMatrixDisplay(confusion_matrix=cm_nb, display_labels=[\"Normal\", \"Spam\"])\n",
        "disp_nb.plot(cmap='Blues', values_format='d')\n",
        "plt.title(\"Confusion Matrix - Naive Bayes\")\n",
        "plt.grid(False)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "tY_o4y_fjL0i"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}